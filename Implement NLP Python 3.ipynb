{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Enthusiastics Statistics Weekend 2017\n",
    "\n",
    "## Intro to Text Processing Using Python\n",
    "#### Module yang akan digunakan adalah re, nltk, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement Natural Language Processing in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Introduction to Natural Language Processing\n",
    "\n",
    "NLP adalah cabang ilmu data (data science) yang terdiri dari proses sistematis untuk menganalisa, memahami, dan memperoleh informasi dari data teks secara cerdas dan efisien. Dengan memanfaatkan NLP dan komponennya, seseorang dapat mengatur potongan besar data teks, melakukan banyak tugas otomatis dan memecahkan berbagai masalah seperti :\n",
    "- automatic summarization (Ringkasan Otomatis),\n",
    "- machine translation (Mesin Terjemahan),\n",
    "- named entity recognition (Pengenalan Entitas),\n",
    "- relationship extraction (Ekstaksi Hubungan), \n",
    "- sentiment analysis (Analsiis Sentimen),\n",
    "- speech recognition (Pengenalan Suara),\n",
    "- topic segmentation (Segmentasi Topik), and etc.\n",
    "\n",
    "Sebelum melangkah lebih jauh, saya ingin menjelaskan beberapa istilah yang akan digunakan:\n",
    "- Tokenisasi  : proses mengubah teks menjadi token\n",
    "- Token       : kata atau entitas yang ada dalam teks\n",
    "- Text Object : kalimat atau frase atau kata atau artikel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Text Preprocessing\n",
    "\n",
    "Teks adalah bentuk yang paling tidak terstruktur dari semua data yang ada, berbagai jenis noise ada di dalamnya dan data tidak mudah dianalisis tanpa pre-processing. Seluruh proses pembersihan dan standardisasi teks, sehingga bebas noise dan siap untuk analisis dikenal sebagai text preprocessing.\n",
    "\n",
    "Hal ini terutama terdiri dari tiga langkah:\n",
    "- Noise Removal\n",
    "- Lexicon Normalization\n",
    "- Object Standardization\n",
    "\n",
    "#### 2.1 Noise Removal\n",
    "\n",
    "Setiap teks yang tidak relevan dengan konteks data dan output akhir, dapat ditentukan sebagai noise.\n",
    "\n",
    "Misalnya - bahasa stopword (kata bahasa yang umum digunakan : adalah, dari, ke, untuk, dll), URL atau tautan, entitas media sosial (mention, hashtag), tanda baca dan kata-kata spesifik industri. Langkah ini berkaitan dengan penghapusan semua jenis entitas noise yang ada dalam teks.\n",
    "\n",
    "Pendekatan umum untuk menghilangkan noise adalah menyiapkan kamus entitas yang noise, dan mengulangi objek teks dengan token (atau dengan kata-kata), menghilangkan token yang ada dalam kamus.\n",
    "\n",
    "Following is the python code for the same purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "noise_list = [\"alias\", \"ini\", \"untuk\", \"...\"] \n",
    "def _remove_noise(input_text):\n",
    "    words = input_text.split()\n",
    "    noise_free_words = [i for i in words if i not in noise_list] \n",
    "    noise_free_text = \" \".join(noise_free_words) \n",
    "    return noise_free_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Enthusiastics Statistics Weekend UII ESW2017 Mahasiswa Statistika UII'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_remove_noise(\"Enthusiastics Statistics Weekend UII alias ESW2017 untuk Mahasiswa Statistika UII\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def _remove_regex(input_text, regex_pattern):\n",
    "    urls = re.finditer(regex_pattern, input_text) \n",
    "    for i in urls: \n",
    "        input_text = re.sub(i.group().strip(), '', input_text)\n",
    "    return input_text\n",
    "\n",
    "regex_pattern = \"#[\\w]*\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Jangan lupa foto yak,    yang heboh'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_remove_regex(\"Jangan lupa foto yak, #hashtag #statisticsuii #esw2017 yang heboh\", regex_pattern)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Lexicon Normalization\n",
    "\n",
    "Tipe noise tekstual lainnya adalah tentang beberapa representasi (ambigue) yang ada dalam satu kata (bukan dari segi makna loh ya).\n",
    "\n",
    "Misalnya - \"bermain\", \"pemain\", \"mainan\", \"mainkan\" dan \"main\" adalah variasi kata yang berbeda - \"main\", Meskipun itu berarti berbeda (makna) tapi secara kontekstual semuanya serupa. Langkah ini mengubah semua perbedaan kata menjadi bentuk dasar/normal mereka (juga dikenal sebagai lemma). Normalisasi adalah langkah penting untuk rekayasa fitur dengan teks karena mengubah fitur dimensi tinggi (fitur yang berbeda) ke ruang berdimensi rendah (1 fitur), yang merupakan model ideal untuk model ML.\n",
    "\n",
    "Praktik normalisasi lexicon yang paling umum adalah:\n",
    "- Stemming: Stemming adalah proses berbasis aturan dasar yang menghilangkan sufiks/imbuhan (\"an\", \"kan\", \"ber\", \"pe\" dll) atau kalau dalam bahasa inggris (“ing”, “ly”, “es”, “s” etc) dari sebuah kata.\n",
    "- Lemmatisasi: Lemmatisasi, di sisi lain, adalah prosedur terorganisir & langkah demi langkah untuk mendapatkan bentuk akar kata, penggunaan kosa kata (kamus penting kata-kata) dan analisis morfologi (struktur kata dan hubungan tata bahasa)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'shop'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word = \"shoping\" \n",
    "\n",
    "from nltk.stem.wordnet import WordNetLemmatizer \n",
    "lem = WordNetLemmatizer()\n",
    "lem.lemmatize(word, \"v\")\n",
    "\n",
    "# FORMAT LEM\n",
    "# for help : lem.lemmatize?\n",
    "# Signature: lem.lemmatize(word, pos='n')\n",
    "# Docstring: <no docstring>\n",
    "# File:      c:\\users\\kofera laptop\\anaconda3\\lib\\site-packages\\nltk\\stem\\wordnet.py\n",
    "# Type:      method\n",
    "# ADJ, ADJ_SAT, ADV, NOUN, VERB = 'a', 's', 'r', 'n', 'v'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'shope'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem.porter import PorterStemmer \n",
    "stem = PorterStemmer()\n",
    "stem.stem(word)\n",
    "\n",
    "# Signature: stem.stem(word)\n",
    "# Docstring:\n",
    "  ## Strip affixes from the token and return the stem.\n",
    "\n",
    "  ## :param token: The token that should be stemmed.\n",
    "  ## :type token: str\n",
    "# File:      c:\\users\\kofera laptop\\anaconda3\\lib\\site-packages\\nltk\\stem\\porter.py\n",
    "# Type:      method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 2.3 Object Standardization\n",
    "\n",
    "Data teks sering mengandung kata atau frasa yang tidak ada dalam kamus leksikal standar manapun. Potongan-potongan ini tidak dikenali oleh search engine dan model.\n",
    "\n",
    "Beberapa contohnya adalah - akronim, hashtag dengan kata-kata terlampir, dan bahasa sehari-hari. Dengan bantuan ungkapan reguler dan kamus data yang disiapkan secara manual, jenis kebisingan ini dapat diperbaiki, kode di bawah ini menggunakan metode pencarian kamus untuk menggantikan slang(bahasa tidak resmi, biasanya musiman) media sosial dari sebuah teks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lookup_dict = {'rt':'Retweet', 'dm':'direct message', \"awsm\" : \"awesome\", \"luv\" :\"love\"}\n",
    "def _lookup_words(input_text):\n",
    "    words = input_text.split() \n",
    "    new_words = [] \n",
    "    for word in words:\n",
    "        if word.lower() in lookup_dict:\n",
    "            word = lookup_dict[word.lower()]\n",
    "        new_words.append(word) \n",
    "        new_text = \" \".join(new_words) \n",
    "        return new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Retweet'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_lookup_words(\"RT Statistika UII hari ini ada EWS loh\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TAMBAHAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc1 = \"ESW UII 2017. Kegiatan yang bertujuan untuk belajar bersama.\" \n",
    "doc2 = \"Statistika UII yang menjadi tuan rumah sekaligus yang mempunyai inisiatif.\"\n",
    "doc3 = \"Berawal dari keinginan untuk memperdalam ilmu dan mengetahui lebih luas dunia data science.\"\n",
    "doc_complete = [doc1, doc2, doc3]\n",
    "doc_clean = [doc.split() for doc in doc_complete]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, '0.073*\"untuk\" + 0.073*\"2017.\" + 0.073*\"Kegiatan\" + 0.073*\"bertujuan\" + 0.073*\"bersama.\" + 0.073*\"belajar\" + 0.073*\"ESW\" + 0.073*\"UII\" + 0.072*\"yang\" + 0.018*\"rumah\"'), (1, '0.121*\"yang\" + 0.069*\"UII\" + 0.069*\"tuan\" + 0.069*\"menjadi\" + 0.069*\"mempunyai\" + 0.069*\"Statistika\" + 0.069*\"inisiatif.\" + 0.069*\"sekaligus\" + 0.069*\"rumah\" + 0.017*\"untuk\"'), (2, '0.060*\"untuk\" + 0.060*\"mengetahui\" + 0.060*\"lebih\" + 0.060*\"luas\" + 0.060*\"science.\" + 0.060*\"dunia\" + 0.060*\"keinginan\" + 0.060*\"dan\" + 0.060*\"memperdalam\" + 0.060*\"data\"')]\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim import corpora\n",
    "\n",
    "# Creating the term dictionary of our corpus, where every unique term is assigned an index.  \n",
    "dictionary = corpora.Dictionary(doc_clean)\n",
    "\n",
    "# Converting list of documents (corpus) into Document Term Matrix using dictionary prepared above. \n",
    "doc_term_matrix = [dictionary.doc2bow(doc) for doc in doc_clean]\n",
    "\n",
    "# Creating the object for LDA model using gensim library\n",
    "Lda = gensim.models.ldamodel.LdaModel\n",
    "\n",
    "# Running and Training LDA model on the document term matrix\n",
    "ldamodel = Lda(doc_term_matrix, num_topics=3, id2word = dictionary, passes=50)\n",
    "\n",
    "# Results \n",
    "hasilldamodel.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UNTUK LEBIH JELASNYA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(6 unique tokens: ['cat', 'dog', 'lion', 'banana', 'orange']...)\n",
      "[[(0, 1), (1, 1)], [(1, 1), (2, 1)], [(0, 1), (2, 1)], [(3, 1), (4, 1)], [(4, 1), (5, 1)], [(3, 1), (5, 1)]]\n",
      "[(0, '0.277*\"lion\" + 0.277*\"cat\" + 0.277*\"dog\" + 0.057*\"banana\" + 0.057*\"orange\" + 0.057*\"apple\"'), (1, '0.277*\"apple\" + 0.277*\"orange\" + 0.277*\"banana\" + 0.057*\"dog\" + 0.057*\"cat\" + 0.057*\"lion\"')]\n"
     ]
    }
   ],
   "source": [
    "doc1 = \"cat dog\" \n",
    "doc2 = \"dog lion\"\n",
    "doc3 = \"cat lion\"\n",
    "doc4 = \"banana orange\" \n",
    "doc5 = \"apple orange\"\n",
    "doc6 = \"banana apple\"\n",
    "doc_complete = [doc1, doc2, doc3,doc4, doc5, doc6]\n",
    "doc_clean = [doc.split() for doc in doc_complete]\n",
    "# Creating the term dictionary of our corpus, where every unique term is assigned an index.  \n",
    "dictionary = corpora.Dictionary(doc_clean)\n",
    "print(dictionary)\n",
    "\n",
    "# Converting list of documents (corpus) into Document Term Matrix using dictionary prepared above. \n",
    "doc_term_matrix = [dictionary.doc2bow(doc) for doc in doc_clean]\n",
    "print(doc_term_matrix)\n",
    "\n",
    "# Creating the object for LDA model using gensim library\n",
    "Lda = gensim.models.ldamodel.LdaModel\n",
    "\n",
    "# Running and Training LDA model on the document term matrix\n",
    "ldamodel = Lda(doc_term_matrix, num_topics=2, id2word = dictionary, passes=50)\n",
    "\n",
    "# Results \n",
    "print(ldamodel.print_topics())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.64273936166093593), (1, 0.35726063833906413)]\n"
     ]
    }
   ],
   "source": [
    "data = \"dog banana dog\"\n",
    "vec = dictionary.doc2bow(data.split())\n",
    "topics_list = ldamodel[vec]\n",
    "print(topics_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
